<html>
<head>
  <title>【Chun Ming Lee】冠军解决方案</title>
  <basefont face="微软雅黑 Light" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/605766 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑 Light;
      font-size: 14pt;
    }
  </style>
</head>
<body>
<a name="7149"/>
<h1>【Chun Ming Lee】冠军解决方案</h1>

<div>
<span><div><div><span style="font-size: unset;"><br/></span></div><div><a href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160862" style="font-size: 18pt; font-family: unset; font-weight: bold;">1st place solution overview</a></div><div>        We’d like to start off by thanking Kaggle/Jigsaw for a drama-free competition and also by congratulating the other medallists.</div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">TL;DR</span></div><ul><li><div>Ensemble, ensemble, ensemble</div></li><li><div>Pseudo-labelling</div></li><li><div><span style="text-decoration: underline;">Bootstrap with multilingual models, refine with monolingual models</span></div></li></ul><div><br/></div><div>        I haven't competed on Kaggle since I co-won the 2018 Toxic Comments competition. The state-of-the-art for NLP classification at the time was non-contextual word embeddings (e.g., FastText) and I was curious if I’d make a good showing in a world of Transformers. I'm pleasantly surprised to have done so well with my team-mate <a href="https://www.kaggle.com/rafiko1">@rafiko1</a>.</div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">Our public LB milestones</span></div><ul><li><div>Baseline XLM-Roberta model (Public LB: 0.93XX - 31 March)</div></li><li><div>Average ensemble of XLM-R models (0.942X - 3 April)</div></li><li><div>Blending with monolingual Transformer models (0.9510 - 23 April)</div></li><li><div>Team merger - weighted average of our individual best subs (0.9529 - 28 April)</div></li><li><div>Blending with monolingual FastText classifiers (0.9544 - 5 May)</div></li><li><div>Post-processing and misc. optimizations (0.9556 - 13 June)</div></li></ul><div><br/></div><div>        You’ll note we hit 1st place on the FINAL public LB by end April. Put another way, we were waiting for 2 months for the competition to end.</div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">CV strategy</span></div><div>        We initially used a mix of k-fold CV and validation set as hold-out, but as we <span style="text-decoration: underline;">refined our test predictions and used pseudo-labels + validation set for training</span>, the validation metric became noisy to the point where we relied primarily on the public LB score.</div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; font-weight: bold;">Insights</span></div><div><span style="font-size: 14pt; font-weight: bold;">Ensembling to mitigate Transformer training variability</span></div><div>        <span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">It’s been noted that the performance of Transformer models is impacted heavily by initialization and data order</span> (<a href="https://arxiv.org/pdf/2002.06305.pdf">https://arxiv.org/pdf/2002.06305.pdf</a>, <a href="https://www.aclweb.org/anthology/2020.trac-1.9.pdf)">https://www.aclweb.org/anthology/2020.trac-1.9.pdf)</a>. <span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">To mitigate that, we emphasized the ensembling and bagging our models.</span> This included temporal self-ensembling. Given the public test set, we went with an <span style="font-weight: bold;">iterative blending approach</span>, refining the test set predictions across submissions with a weighted average of the previous best submission and the current model’s predictions. We began with a simple average, and gradually increased the weight of the previous best submission. For the training data, we largely used sub-samples of the translations of the 2018 toxic comments for each model run.（不要轻易改变高分集成模型内部权重的比例，而是将迭代式的集成；同时作者使用时序自集成，将每个epoch的预测结果平均，这消除了找出最佳迭代次数的需要）</div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">Pseudo-labels (PL)</span></div><div>        <span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">We observed a performance improvement when we used test-set predictions as training data - the intuition being that it helps models learn the test set distribution. Using all test-set predictions as soft-labels worked better than any other version of pseudo-labelling (e.g., hard labels, confidence thresholded PLs etc.).</span> Towards the end of the competition, we discovered a minor but material boost in LB when we upsampled the PLs.</div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">Multilingual XLM-Roberta models</span></div><div>        As with most teams, we began with a vanilla XLM-R model, incorporating translations of the 2018 dataset in the 6 test-set languages as training data. We used a vanilla classification head on the CLS token of the last layer with the Adam optimizer and binary cross entropy loss function, and finetuned the entire model with a low learning rate. <span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">Given Transformer models have several hundred million trainable weights put to the relatively simple task of making a binary prediction, we didn’t believe the default architecture required much tweaking to get a good signal through the model. Consequently, we didn't spend too much time on hyper-parameter optimization, architectural tweaks, or preprocessing.</span></div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">Foreign language monolingual Transformer models</span></div><div>        <a href="https://www.kaggle.com/rafiko1">@rafiko1</a> and I stumbled on this technique independently and prior to forming a team. We were both inspired by the MultiFiT paper (<a href="https://arxiv.org/pdf/1909.04761.pdf">https://arxiv.org/pdf/1909.04761.pdf</a>) - specifically: </div><div style="text-align: center;"><img src="【Chun Ming Lee】冠军解决方案_files/inbox_1029053_b40975c9453fba6f640836e4fe297746_Capture.png" type="image/png" data-filename="inbox_1029053_b40975c9453fba6f640836e4fe297746_Capture.png" width="477"/></div><div>        <span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">We observed a dramatic performance boost when we used pretrained foreign language monolingual Transformer models from HuggingFace for the test-set languages(e.g., Camembert for french samples, Rubert for russian, BerTurk for turkish, BETO for spanish etc.</span><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">).（ A training run would for example have 10K test + 70K subsampled 2018 translations + 2.5K val. ）</span></div><div><br/></div><div>        We finetuned models for each of the 6 languages - by combining translations of the 2018 Toxic Comments together with pseudo-labels for samples in that specific language (initially from the XLM-R multilingual models), training the corresponding monolingual model, predicting the same samples then blending it back with the “main branch” of all predictions. It was synergistic in that training model A -&gt; training model B -&gt; training model A etc. lead to continual performance improvements.</div><div><br/></div><div>        For each model run, we’d reload weight initalizations from the pretrained models to prevent overfitting. In other words, the continuing improvements we saw were being driven by refinements in the pseudo-labels we were providing to the models as training data. For a given monolingual model, predicting only test-set samples in that language worked best. Translating test-set samples in other languages to the model's language and predicting them worsened performance.</div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">Finetuning pre-trained foreign language monolingual FastText models</span></div><div>        After we exhausted the HuggingFace monolingual model library, we trained monolingual FastText Bidirectional GRU models a la 2018 Toxic Comments, using pretrained embeddings for the test-set languages, to continue refining the test set predictions (albeit with a lower weight when combined with the main branch of predictions) and saw a small but meaningful performance boost (0.9536 to 0.9544, For FastText, we used official pretrained embeddings in the 6 test set languages so didn't touch English data).</div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">Post-processing</span></div><div>        Given the large number of submissions we were making, <a href="https://www.kaggle.com/rafiko1">@rafiko1</a> came up with the novel idea to make use of the history of submissions to tweak our test set predictions. We tracked the delta of predictions for each sample for successful submissions, averaged them and nudged the predictions in the same direction. We saw a minor but material boost in performance (~0.0005)</div><div><br/></div><div>        We were concerned about the risk of overfitting with this post-processing technique so for our final 2 submissions, selected one that incorporated this post-processing and one that didn't. It ended up working very well on the private LB.</div><div><br/></div><div><br/></div><div><br/></div><div style="text-align: left;"><span style="font-size: 18pt; font-weight: bold;">Misc</span></div><div><span style="font-size: 14pt; font-weight: bold;">Training setup</span></div><div>        <a href="https://www.kaggle.com/rafiko1">@rafiko1</a> and I kept separate code-bases. He trained on Kaggle TPU instances with Tensorflow code derived from the public kernels by <a href="https://www.kaggle.com/xhlulu">@xhlulu</a> and <a href="https://www.kaggle.com/shonenkov">@shonenkov</a> while I trained on my own hardware (dual RTX Titans) using from-scratch PyTorch code. I’d attribute part of our outsized merger ensemble boost (0.9510 -&gt; 0.9529) to this diversity in training.（不同的深度学习框架会带来提升）</div><div><br/></div><div><br/></div><div><span style="font-size: 14pt; font-weight: bold;">What didn’t work</span></div><div>        We went through a checklist of contemporary NLP classification techniques and most of them didn’t work probably due to the simple problem objective. Here is a selection of what didn’t work:</div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Document-level embedders (e.g., LASER)</span></div><div style="margin-left: 40px;">- Further MLM pretraining of Transformer models using task data</div><div style="margin-left: 40px;">- Alternative ensembling mechanisms (rank-averaging, stochastic weight averaging)</div><div style="margin-left: 40px;">- Alternative loss functions (e.g., focal loss, <span style="text-decoration: underline;">histogram loss</span>)</div><div style="margin-left: 40px;">- Alternative pooling mechanisms for the classification head (e.g., <span style="text-decoration: underline;">max-pool CNN across tokens</span>, using multiple hidden layers etc.)</div><div style="margin-left: 40px;">- Non-FastText pretrained embeddings (e.g., Flair, glove, bpem)</div><div style="margin-left: 40px;">- Freeze-finetuning for the Transformer models</div><div style="margin-left: 40px;">- Regularization (e.g., <span style="text-decoration: underline;">multisample dropout, input mixup, manifold mixup, sentencepiece-dropout</span>)</div><div style="margin-left: 40px;">- Backtranslation as data augmentation</div><div style="margin-left: 40px;">- English translations as train/test-time augmentation</div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Self-distilling to relabel the 2018 data</span></div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Adversarial training by perturbing the embeddings layer using FGM</span></div><div style="margin-left: 40px;">- Multi-task learning</div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Temperature scaling on pseudo-labels</span></div><div style="margin-left: 40px;">- Semi-supervised learning using the test data</div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Composing two models into an encoder-decoder model</span></div><div style="margin-left: 40px;">- <span style="text-decoration: underline;">Use of translation focused pretrained models (e.g., mBart)</span></div><div><br/></div><div>        We'll release our code later this week.</div><div><br/></div></div><div><br/></div></span>
</div></body></html> 